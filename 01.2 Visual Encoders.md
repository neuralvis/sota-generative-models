# 01.2 Visual Encoders

There seems to primarily be three different paradigms for visual encoders -

1. [CLIP](https://openai.com/index/clip/), [SIGLIP](https://arxiv.org/pdf/2502.14786?) style encoders that encodes visual concepts (semantically) from language in a common latent space for both images and text. 
2. [DINOv3](https://ai.meta.com/dinov3/) like models that encode pixel-level 3d structures, segmentation information etc. on input images. Note that DINOv3
3. Point-cloud based approach for dense prediction tasks such as [AutoFocusFormer](https://arxiv.org/abs/2304.12406) - [Github](https://github.com/apple/ml-autofocusformer), and [Semantic SLAM](https://mingqij.github.io/projects/gs4/) that seems to be doing dense prediction tasks for robotics vision etc. 


## Questions

- Point 3 seems to create a dense 3d map of the world around them as they go. Would this be most useful for a world model ? I.e. they create the most dense spatial understanding. 
- Need to see if CUT3R and VGGT papers are in the same category as #3. 

