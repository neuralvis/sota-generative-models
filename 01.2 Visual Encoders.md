# 01.2 Visual Encoders and Decoders



## Understanding vs. Generation
- Image Understanding
	- Requires high-level semantic understanding and reasoning
	- Vision encoder focuses on high-dimensional representation to capture this
- Image Generation
	- Generate local details while maintaining global consistency
	- Encoder needs low-dimensional encoding for fine-grained spatial structure/texture
- Unifying both directly will lead to conflicts and trade-offs
- Models that combine image understanding and image generation into the same backbone (eg: Janus).
- Models that combine Image and Video Generation into the same backbone: eg. Goku by ByteDance
- Using Multimodal models for training
	- Goku uses a VLM Tarsier2
	- Janus-pro uses ?? Models for prompt alignment and training
	- Cosmos uses something ?
- Using autoregressive models

## Introduction
There seems to primarily be three different paradigms for visual encoders -

1. [CLIP](https://openai.com/index/clip/), [SIGLIP](https://arxiv.org/pdf/2502.14786?) style encoders that encodes visual concepts (semantically) from language in a common latent space for both images and text. 
2. [DINOv3](https://ai.meta.com/dinov3/) like models that encode pixel-level 3d structures, segmentation information etc. on input images. Note that DINOv3
3. Point-cloud based approach for dense prediction tasks such as [AutoFocusFormer](https://arxiv.org/abs/2304.12406) - [Github](https://github.com/apple/ml-autofocusformer), and [Semantic SLAM](https://mingqij.github.io/projects/gs4/) that seems to be doing dense prediction tasks for robotics vision etc. 


## Questions

- Point 3 seems to create a dense 3d map of the world around them as they go. Would this be most useful for a world model ? I.e. they create the most dense spatial understanding. 
- Need to see if CUT3R and VGGT papers are in the same category as #3. 


## Representations

- V-JEPA 2 ? What does it do ? 
- The tension between encoding semantics vs. encoding spatial understanding
	- The Janus-Pro paper articulates this clearly. 
- Fuxin’s work
	- https://mingqij.github.io/projects/gs4/
	- https://github.com/apple/ml-autofocusformer


There seems to be two disconnected paradigms for “visual encodings”

1. High level semantic encoding (for text alignment, captioning etc.) - by SIGLIP and CLIP - like Models. Requires high level semantic understanding and reasoning. Encoding task focuses on high-dimensional representations to capture this. 
2. Providing low-level encoding (for perception and generation tasks): The family of VAEs (VQ-VAE, etc) and the likes of DINOv3 fall into this category. 

The original Janus paper - https://arxiv.org/abs/2410.13848 articulates and discusses this conceptually. 

Where does VJEPA 2 (by meta) fall in this mental model ?
Janus says we cannot unify these representations. Why ? What are the fundamental issues with doing this ? 

Do these papers attempt to address this issue ?

- [C. Ziwen et al. AutoFocusFormer: Image Segmentation off the Grid. CVPR 2023.](https://github.com/apple/ml-autofocusformer)
- [GS4: Generalizable Sparse Splatting Semantic SLAM](https://mingqij.github.io/projects/gs4/). ? 

## Recent SoTA 
Dated October 21, 2025

The following papers use more grounded 3D latents - 

1. [AToken](https://machinelearning.apple.com/research/atoken) - A unified tokenizer for Vision
2. [SVG](https://howlin-wang.github.io/svg/) Latent diffusion model without Variational Autoencoder
3. [REPA](https://rae-dit.github.io) - Diffusion Transformers with Representation Autoencoders


