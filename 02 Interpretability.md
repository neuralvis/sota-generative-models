# Understanding Latent Spaces

## Readings

### Random
https://www.jeremyjordan.me/variational-autoencoders/

https://medium.com/@weidagang/demystifying-neural-networks-variational-autoencoders-6a44e75d0271

**https://medium.com/data-science/understanding-variational-autoencoders-vaes-f70510919f73**

https://medium.com/@rushikesh.shende/autoencoders-variational-autoencoders-vae-and-β-vae-ceba9998773d

https://www.youtube.com/@Deepia-ls2fo

https://www.tilderesearch.com/blog/rate-distortion-saes

https://avandekleut.github.io/vae/

### Sparse Autoencoders

- SAEdit - https://ronen94.github.io/SAEdit/

### Random but useful VAE Nuggets
- Some intuitions about the VAE Encoders are articulated [here](https://x.com/sang_yun_lee/status/1912548035717931304) and reference the following works
	- [Deep Compression Autoencoder for Efficient High-Resolution Diffusion Models](https://arxiv.org/abs/2410.10733)
	- [Diagnosing and Enhancing VAE models](https://arxiv.org/abs/1903.05789)
- Series of posts by Rudy Gilman on VAEs
	- *[The attention layers in the VAEs for FLUX, Stable Diffusion 3.5, and SDXL don't do anything.](https://x.com/rgilman33/status/1914273430611906590)*
	- [*Here's broadly how I think the SDXL-VAE escapes the "tyranny of the grid”.*](https://x.com/rgilman33/status/1912206589173571616)

### Good Technical Blogs

- [Generative Modeling in Latent Space by Sander Dielman](https://github.com/hustvl/LightningDiT) - Talks extensively about latents
- [Deep representation learning](https://ma-lab-berkeley.github.io/deep-representation-learning-book/)

### Anthropic Research on Interpretability
- https://www.anthropic.com/research#interpretability
- https://transformer-circuits.pub
- [Circuit Tracing Tool](https://www.anthropic.com/research/open-source-circuit-tracing)
- https://transformer-circuits.pub/2025/attribution-graphs/methods.html

### Disentangling Latents
Sliderspace - https://sliderspace.baulab.info

## Tools and Models

### Visualizing Latents
- **[DarkSpark](https://darkspark.dev)**
- https://github.com/jessevig/bertviz
- https://github.com/guoqincode/DiT-Visualization


- ComfyUI + Latent Visualization
	- https://colab.research.google.com/drive/1176r3ZVqqxzVoj1pfq068_ehnLYdWoxm#scrollTo=Q0V1MtewwhZo
	- https://x.com/prathyvsh/status/1852215239850225943

- https://github.com/mashaan14/VisionTransformer-MNIST/blob/main/VisionTransformer_MNIST.ipynb

- https://x.com/msfeldstein/status/1849518352059990428
	- Code: [Sliding in Latent Space](https://colab.research.google.com/drive/1176r3ZVqqxzVoj1pfq068_ehnLYdWoxm#scrollTo=Q0V1MtewwhZo)

- https://x.com/michaelwhanna/status/1928122114726297812
	- Code: https://www.neuronpedia.org


### Visualizing Diffusion and Flow Generative Models

- https://github.com/helblazer811/Diffusion-Explorer

### Hash Grid Encoding
- https://github.com/Ending2015a/hash-grid-encoding
- https://sair.synerise.com/emde-vs-multiresolution-hash-encoding/
- https://sair.synerise.com/emde-illustrated/

### New Latents
- V-Jepa by Meta
- Causal Encoder - by Wan2.1 

## Datasets
- https://jialuo-li.github.io/Science-T2I-Web/
	- Can we use datasets from here to see what the latents look like ?